{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "from __future__ import unicode_literals\n",
    "import codecs\n",
    "import os\n",
    "import re\n",
    "\n",
    "PKU_TRAIN='original_data/pku_training.utf8'\n",
    "PKU_TEST='original_data/pku_test_gold.utf8'\n",
    "MSR_TRAIN='original_data/msr_training.utf8'\n",
    "MSR_TEST='original_data/msr_test_gold.utf8'\n",
    "AS_TRAIN='original_data/as_training.utf8'\n",
    "AS_TEST='original_data/as_test_gold.utf8'\n",
    "CITYU_TRAIN='original_data/cityu_training.utf8'\n",
    "CITYU_TEST='original_data/cityu_test_gold.utf8'\n",
    "CTB_TRAIN='original_data/ctb_train'\n",
    "CTB_DEV='original_data/ctb_dev'\n",
    "CTB_TEST='original_data/ctb_test'\n",
    "CHINESE_IDIOMS='original_data/idioms'\n",
    "OUTPUT_PATH='data'\n",
    "\n",
    "rNUM = '(-|\\+)?\\d+((\\.|·)\\d+)?%?'\n",
    "rENG = '[A-Za-z_.]+'\n",
    "START='S'\n",
    "END='E'\n",
    "\n",
    "def strQ2B(ustring):\n",
    "    \"\"\"全角转半角\"\"\"\n",
    "    rstring = \"\"\n",
    "    for uchar in ustring:\n",
    "        inside_code = ord(uchar)\n",
    "        if inside_code == 12288:  # 全角空格直接转换\n",
    "            inside_code = 32\n",
    "        elif (inside_code >= 65281 and inside_code <= 65374):  # 全角字符（除空格）根据关系转化\n",
    "            inside_code -= 65248\n",
    "        rstring += unichr(inside_code)\n",
    "    return rstring\n",
    "\n",
    "\n",
    "def preprocess(input,output):\n",
    "    ''' pku_training.utf8(input) 生成一个pku_train_all(output)文件 '''\n",
    "    output_filename = os.path.join(OUTPUT_PATH,output)\n",
    "    idioms=dict()\n",
    "    #取出成语词典中的成语\n",
    "    with codecs.open(CHINESE_IDIOMS,'r','utf-8') as f:    \n",
    "        for line in f:  \n",
    "            idioms[line.strip()]=1    #返回移除字符串头尾指定的字符(空格)生成的新字符串                              标记为1\n",
    "    count_idioms = 0\n",
    "    sents=[]\n",
    "    #pku_training.utf8的文本处理：\n",
    "    with codecs.open(input,'r','utf-8') as fin:\n",
    "        with codecs.open(output_filename,'w','utf-8') as fout:\n",
    "            for line in fin:   #取pku_training 的每一行\n",
    "                sent=strQ2B(line).split( ) #此行 词 的集合\n",
    "                new_sent=[]\n",
    "                for word in sent:\n",
    "                    word=re.sub(rNUM,'0',word)  #正则 没懂\n",
    "                    word=re.sub(rENG,'X',word)\n",
    "                    if idioms.get(word) is not None:\n",
    "                        count_idioms+=1\n",
    "                        word=u'I'\n",
    "                    new_sent.append(word)\n",
    "                sents.append(new_sent)\n",
    "            for sent in sents:\n",
    "                fout.write('  '.join(sent))\n",
    "                fout.write('\\n')\n",
    "    #print 'idioms count:%d' % count_idioms\n",
    "\n",
    "def split(dataset): #划分出验证集\n",
    "#dataset='pku'\n",
    "    dataset=os.path.join(OUTPUT_PATH,dataset)\n",
    "    with codecs.open(dataset+'_train_all','r','utf-8') as f:\n",
    "\t    lines = f.readlines()\n",
    "\t    idx = int(len(lines)*0.9)\n",
    "\t    with codecs.open(dataset+'_train','wb','utf-8') as fo:\n",
    "\t\t    for line in lines[:idx]:\n",
    "\t\t\t    fo.write(line.strip()+'\\r')\n",
    "\t    with codecs.open(dataset+'_dev','wb','utf-8') as fo:\n",
    "\t\t    for line in lines[idx:]:\n",
    "\t\t\t    fo.write(line.strip()+'\\r')\n",
    "    os.remove(dataset+'_train_all')\n",
    "\n",
    "\n",
    "def ngram(ustr,n=2):\n",
    "    ngram_list=[]\n",
    "    for i in range(len(ustr)-n+1):\n",
    "        ngram_list.append(ustr[i:i+n])\n",
    "    return ngram_list\n",
    "\n",
    "def bigram_words(dataset,window_size=2):\n",
    "#dataset=‘pku_train’\n",
    "    dataset=os.path.join(OUTPUT_PATH,dataset)\n",
    "    words=dict()   \n",
    "    start=''.join([START]*window_size)   # start=‘SS’\n",
    "    end=''.join([END]*window_size)      \n",
    "    with codecs.open(dataset,'r','utf-8') as f:\n",
    "        for line in f:\n",
    "            line=start+re.sub('\\s+','',line.strip())+end\n",
    "            for word in ngram(line,window_size):\n",
    "                words[word]=words.get(word,0)+1\n",
    "    with codecs.open(dataset+'_bigram','w','utf-8') as f:\n",
    "        for k,v in words.items():\n",
    "            f.write(k+' '+unicode(v)+'\\n')\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    if not os.path.exists(OUTPUT_PATH):\n",
    "        os.mkdir(OUTPUT_PATH)\n",
    "    #preprocess\n",
    "    print 'start preprocess'\n",
    "    preprocess(PKU_TRAIN,'pku_train_all') #pku_training.utf8 生成一个pku_train_all文件\n",
    "    preprocess(PKU_TEST,'pku_test')\n",
    "    preprocess(MSR_TRAIN,'msr_train_all')\n",
    "    preprocess(MSR_TEST,'msr_test')\n",
    "    preprocess(AS_TRAIN,'as_train_all')\n",
    "    preprocess(AS_TEST,'as_test')\n",
    "    preprocess(CITYU_TRAIN,'cityu_train_all')\n",
    "    preprocess(CITYU_TEST,'cityu_test')\n",
    "    # preprocess(CTB_TRAIN,'ctb_train')\n",
    "    # preprocess(CTB_DEV,'ctb_dev')\n",
    "    # preprocess(CTB_TEST,'ctb_test')\n",
    "    #split\n",
    "    print 'start split'\n",
    "    split('pku')   #划分出验证集,pku_train_all变为pku_train和pku_dev\n",
    "    split('msr')\n",
    "    split('as')\n",
    "    split('cityu')\n",
    "    #bigram\n",
    "    print 'start bigram'\n",
    "    bigram_words('pku_train')\n",
    "    bigram_words('msr_train')\n",
    "    bigram_words('as_train')\n",
    "    bigram_words('cityu_train')\n",
    "    # bigram_words('ctb_train')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py27",
   "language": "python",
   "name": "py27"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
