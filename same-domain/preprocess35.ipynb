{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "from __future__ import unicode_literals\n",
    "import codecs\n",
    "import os\n",
    "import re\n",
    "\n",
    "BALA_TRAIN='original_data/bala_training.utf8'\n",
    "BALA_TEST='original_data/bala_training.utf8'\n",
    "\n",
    "CHINESE_IDIOMS='original_data/idioms'\n",
    "OUTPUT_PATH='data'\n",
    "\n",
    "rNUM = '(-|\\+)?\\d+((\\.|·)\\d+)?%?'\n",
    "rENG = '[A-Za-z_.]+'\n",
    "START='S'\n",
    "END='E'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def strQ2B(ustring):\n",
    "    \"\"\"全角转半角\"\"\"\n",
    "    rstring = \"\"\n",
    "    for uchar in ustring:\n",
    "        inside_code = ord(uchar)\n",
    "        if inside_code == 12288:  # 全角空格直接转换\n",
    "            inside_code = 32\n",
    "        elif (inside_code >= 65281 and inside_code <= 65374):  # 全角字符（除空格）根据关系转化\n",
    "            inside_code -= 65248\n",
    "        #rstring += unichr(inside_code)\n",
    "        rstring += chr(inside_code)\n",
    "        \n",
    "    return rstring\n",
    "\n",
    "\n",
    "def preprocess(input,output):\n",
    "    ''' pku_training.utf8(input) 生成一个pku_train_all(output)文件 '''\n",
    "    output_filename = os.path.join(OUTPUT_PATH,output)\n",
    "    idioms=dict()\n",
    "    #取出成语词典中的成语\n",
    "    with codecs.open(CHINESE_IDIOMS,'r','utf-8') as f:    \n",
    "        for line in f:  \n",
    "            idioms[line.strip()]=1    #返回移除字符串头尾指定的字符(空格)生成的新字符串                              标记为1\n",
    "    count_idioms = 0\n",
    "    sents=[]\n",
    "    #pku_training.utf8的文本处理：\n",
    "    with codecs.open(input,'r','utf-8') as fin:\n",
    "        with codecs.open(output_filename,'w','utf-8') as fout:\n",
    "            for line in fin:   #取pku_training 的每一行\n",
    "                sent=strQ2B(line).split( ) #此行 词 的集合\n",
    "                new_sent=[]\n",
    "                for word in sent:\n",
    "                    word=re.sub(rNUM,'0',word)  #正则 没懂\n",
    "                    word=re.sub(rENG,'X',word)\n",
    "                    if idioms.get(word) is not None:\n",
    "                        count_idioms+=1\n",
    "                        word=u'I'\n",
    "                    new_sent.append(word)\n",
    "                sents.append(new_sent)\n",
    "            for sent in sents:\n",
    "                fout.write('  '.join(sent))\n",
    "                fout.write('\\n')\n",
    "    #print 'idioms count:%d' % count_idioms\n",
    "\n",
    "def split(dataset): #划分出验证集\n",
    "#dataset='pku'\n",
    "    dataset=os.path.join(OUTPUT_PATH,dataset)\n",
    "    with codecs.open(dataset+'_train_all','r','utf-8') as f:\n",
    "\t    lines = f.readlines()\n",
    "\t    idx = int(len(lines)*0.9)\n",
    "\t    with codecs.open(dataset+'_train','wb','utf-8') as fo:\n",
    "\t\t    for line in lines[:idx]:\n",
    "\t\t\t    fo.write(line.strip()+'\\r')\n",
    "\t    with codecs.open(dataset+'_dev','wb','utf-8') as fo:\n",
    "\t\t    for line in lines[idx:]:\n",
    "\t\t\t    fo.write(line.strip()+'\\r')\n",
    "    os.remove(dataset+'_train_all')\n",
    "\n",
    "\n",
    "def ngram(ustr,n=2):\n",
    "    ngram_list=[]\n",
    "    for i in range(len(ustr)-n+1):\n",
    "        ngram_list.append(ustr[i:i+n])\n",
    "    return ngram_list\n",
    "\n",
    "def bigram_words(dataset,window_size=2):\n",
    "#dataset=‘pku_train’\n",
    "    dataset=os.path.join(OUTPUT_PATH,dataset)\n",
    "    words=dict()   \n",
    "    start=''.join([START]*window_size)   # start=‘SS’\n",
    "    end=''.join([END]*window_size)      \n",
    "    with codecs.open(dataset,'r','utf-8') as f:\n",
    "        for line in f:\n",
    "            line=start+re.sub('\\s+','',line.strip())+end\n",
    "            for word in ngram(line,window_size):\n",
    "                words[word]=words.get(word,0)+1\n",
    "    with codecs.open(dataset+'_bigram','w','utf-8') as f:\n",
    "        for k,v in words.items():\n",
    "            f.write(k+' '+str(v)+'\\n')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "start preprocess\n",
      "start split\n",
      "start bigram\n"
     ]
    }
   ],
   "source": [
    "if not os.path.exists(OUTPUT_PATH):\n",
    "    os.mkdir(OUTPUT_PATH)\n",
    "#preprocess\n",
    "print(\"start preprocess\")\n",
    "\n",
    "preprocess(BALA_TRAIN,'bala_train_all') #pku_training.utf8 生成一个pku_train_all文件\n",
    "preprocess(BALA_TEST,'bala_test')\n",
    "\n",
    "\n",
    "#split\n",
    "print(\"start split\")\n",
    "split('bala')   #划分出验证集,pku_train_all变为pku_train和pku_dev\n",
    "\n",
    "#bigram\n",
    "print(\"start bigram\")\n",
    "bigram_words('bala_train')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py35",
   "language": "python",
   "name": "py35"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
